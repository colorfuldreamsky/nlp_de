{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell checker \n",
    "\n",
    "We can add new words to the spell checker or use the spell checker directly for other languages than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker() # loads default word frequency list\n",
    "#spell.word_frequency.load_text_file('meine_datei.txt')\n",
    "\n",
    "# if I just want to make sure some words are not flagged as misspelled\n",
    "spell.word_frequency.load_words(['Nürnberg','München'])\n",
    "spell.known(['Nürnberg','München'])  # will return both now!\n",
    "\n",
    "spell_de = SpellChecker(language='de', case_sensitive=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "Load all German cities from the ``de.csv`` into the spell checker. You need to parse the CSV file with the csv Python package. The data can be found at [https://simplemaps.com/data/de-cities](https://simplemaps.com/data/de-cities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "city_list = []\n",
    "for city in cities:\n",
    "    pass\n",
    "spell_de.known([city_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German word2vec\n",
    "\n",
    "Spacy works with many other languages like for German, French, and Spanish. The full list can be found at [https://spacy.io/usage/models](https://spacy.io/usage/models). Let's take a look how it works and compare it with the English version. Please keep in mind that we use the small version of the model. You can always download a more precise model for German like [https://spacy.io/models/de](https://spacy.io/models/de)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king king 1.0\n",
      "king queen 0.49073455\n",
      "king horse 0.33970487\n",
      "king cat 0.3899064\n",
      "king lamp 0.39127484\n",
      "queen king 0.49073455\n",
      "queen queen 1.0\n",
      "queen horse 0.46672326\n",
      "queen cat 0.5033449\n",
      "queen lamp 0.38795537\n",
      "horse king 0.33970487\n",
      "horse queen 0.46672326\n",
      "horse horse 1.0\n",
      "horse cat 0.3925663\n",
      "horse lamp 0.415988\n",
      "cat king 0.3899064\n",
      "cat queen 0.5033449\n",
      "cat horse 0.3925663\n",
      "cat cat 1.0\n",
      "cat lamp 0.3361321\n",
      "lamp king 0.39127484\n",
      "lamp queen 0.38795537\n",
      "lamp horse 0.415988\n",
      "lamp cat 0.3361321\n",
      "lamp lamp 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "tokens = nlp(u'king queen horse cat lamp')\n",
    "\n",
    "for first_token in tokens:\n",
    "    for second_token in tokens:\n",
    "        print(first_token.text, second_token.text, first_token.similarity(second_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "könig könig 1.0\n",
      "könig königen 0.37712693\n",
      "könig pferd 0.5495152\n",
      "könig katze 0.508159\n",
      "könig lampe 0.26994777\n",
      "königen könig 0.37712693\n",
      "königen königen 1.0\n",
      "königen pferd 0.28978667\n",
      "königen katze 0.6135397\n",
      "königen lampe 0.37072814\n",
      "pferd könig 0.5495152\n",
      "pferd königen 0.28978667\n",
      "pferd pferd 1.0\n",
      "pferd katze 0.46782535\n",
      "pferd lampe 0.3315421\n",
      "katze könig 0.508159\n",
      "katze königen 0.6135397\n",
      "katze pferd 0.46782535\n",
      "katze katze 1.0\n",
      "katze lampe 0.29568455\n",
      "lampe könig 0.26994777\n",
      "lampe königen 0.37072814\n",
      "lampe pferd 0.3315421\n",
      "lampe katze 0.29568455\n",
      "lampe lampe 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de\")\n",
    "\n",
    "tokens = nlp(u'könig königen pferd katze lampe')\n",
    "\n",
    "for first_token in tokens:\n",
    "    for second_token in tokens:\n",
    "        print(first_token.text, second_token.text, first_token.similarity(second_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"Es war einmal ein kleines süßes Mädchen, das hatte jedermann lieb, der sie nur ansah, am allerliebsten aber ihre Großmutter, die wusste gar nicht, was sie alles dem Kinde geben sollte.\")\n",
    "doc2 = nlp(u\"Einmal schenkte sie ihm ein Käppchen von rotem Samt, und weil ihm das so wohl stand, und es nichts anders mehr tragen wollte, hieß es nur das Rotkäppchen.\")\n",
    "doc3 = nlp(u\"Eines Tages sprach seine Mutter zu ihm: Komm, Rotkäppchen, da hast du ein Stück Kuchen und eine Flasche Wein, bring das der Großmutter hinaus; sie ist krank und schwach und wird sich daran laben.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Named Entitiy Recognition training\n",
    "\n",
    "Based on [https://spacy.io/usage/training](https://spacy.io/usage/training), we can train the model with NER entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'de'\n",
      "Losses {'ner': 9.517211854457855}\n",
      "Losses {'ner': 9.851449131965637}\n",
      "Losses {'ner': 5.231690675318276}\n",
      "Losses {'ner': 9.805707931518555}\n",
      "Losses {'ner': 6.0083559161710625}\n",
      "Losses {'ner': 4.325909856152975}\n",
      "Losses {'ner': 6.56257762936616}\n",
      "Losses {'ner': 5.230663496709894}\n",
      "Losses {'ner': 6.867360518317582}\n",
      "Losses {'ner': 8.012482523918152}\n",
      "Losses {'ner': 6.320870776597961}\n",
      "Losses {'ner': 6.256802976131439}\n",
      "Losses {'ner': 5.434958373902415}\n",
      "Losses {'ner': 6.349413275718689}\n",
      "Losses {'ner': 4.2376793867958575}\n",
      "Losses {'ner': 7.630850732326508}\n",
      "Losses {'ner': 4.1439585315024825}\n",
      "Losses {'ner': 2.1816334187130115}\n",
      "Losses {'ner': 8.86284452676773}\n",
      "Losses {'ner': 3.8126182841625385}\n",
      "Losses {'ner': 6.962542176246643}\n",
      "Losses {'ner': 4.545411241294438}\n",
      "Losses {'ner': 6.70014488697052}\n",
      "Losses {'ner': 8.526988863945007}\n",
      "Losses {'ner': 6.621102124452591}\n",
      "Losses {'ner': 6.189238959038657}\n",
      "Losses {'ner': 4.659776524218387}\n",
      "Losses {'ner': 4.672071951588805}\n",
      "Losses {'ner': 7.964940910325716}\n",
      "Losses {'ner': 3.277584773060312}\n",
      "Losses {'ner': 3.5400431088020925}\n",
      "Losses {'ner': 3.2837426677417016}\n",
      "Losses {'ner': 5.753972954904256}\n",
      "Losses {'ner': 8.717468798160553}\n",
      "Losses {'ner': 6.586069047451019}\n",
      "Losses {'ner': 8.651286005973816}\n",
      "Losses {'ner': 5.943768910005019}\n",
      "Losses {'ner': 6.0263583064079285}\n",
      "Losses {'ner': 6.5555365783257}\n",
      "Losses {'ner': 4.784536913209877}\n",
      "Losses {'ner': 5.5587350428832}\n",
      "Losses {'ner': 7.301437735557556}\n",
      "Losses {'ner': 2.7182678782046423}\n",
      "Losses {'ner': 4.781375553095131}\n",
      "Losses {'ner': 3.3885957970345397}\n",
      "Losses {'ner': 5.653118133544922}\n",
      "Losses {'ner': 7.088170945644379}\n",
      "Losses {'ner': 4.164064388029601}\n",
      "Losses {'ner': 6.689637961428161}\n",
      "Losses {'ner': 4.101142876548693}\n",
      "Losses {'ner': 5.486014919930135}\n",
      "Losses {'ner': 6.220814565840556}\n",
      "Losses {'ner': 7.4143847823143005}\n",
      "Losses {'ner': 4.5360502187977545}\n",
      "Losses {'ner': 4.5396723687772464}\n",
      "Losses {'ner': 5.6494021108374}\n",
      "Losses {'ner': 4.591360252123195}\n",
      "Losses {'ner': 4.693824564368697}\n",
      "Losses {'ner': 3.0959486509673297}\n",
      "Losses {'ner': 2.176613037718255}\n",
      "Losses {'ner': 4.5331467199430335}\n",
      "Losses {'ner': 6.294409162452212}\n",
      "Losses {'ner': 5.713100000061814}\n",
      "Losses {'ner': 9.028004944324493}\n",
      "Losses {'ner': 5.484868049621582}\n",
      "Losses {'ner': 6.4324411153793335}\n",
      "Losses {'ner': 2.2950605140649714}\n",
      "Losses {'ner': 6.423489379580133}\n",
      "Losses {'ner': 3.608826240122653}\n",
      "Losses {'ner': 2.538939370540902}\n",
      "Losses {'ner': 4.33433829947171}\n",
      "Losses {'ner': 8.007838634599466}\n",
      "Losses {'ner': 3.6865342514938675}\n",
      "Losses {'ner': 8.417553126811981}\n",
      "Losses {'ner': 3.5190893136095838}\n",
      "Losses {'ner': 4.428861978318309}\n",
      "Losses {'ner': 2.5212067518004915}\n",
      "Losses {'ner': 4.9694231590256095}\n",
      "Losses {'ner': 4.109288083738647}\n",
      "Losses {'ner': 6.6093005537986755}\n",
      "Losses {'ner': 4.854203753347974}\n",
      "Losses {'ner': 0.9631222463212907}\n",
      "Losses {'ner': 6.311730194371194}\n",
      "Losses {'ner': 8.211120367050171}\n",
      "Losses {'ner': 4.521150782704353}\n",
      "Losses {'ner': 6.116706885484746}\n",
      "Losses {'ner': 4.052567769540474}\n",
      "Losses {'ner': 6.501973509788513}\n",
      "Losses {'ner': 4.519970728084445}\n",
      "Losses {'ner': 1.8701871695229784}\n",
      "Losses {'ner': 5.665457189083099}\n",
      "Losses {'ner': 4.090357391163707}\n",
      "Losses {'ner': 2.419262232258916}\n",
      "Losses {'ner': 5.620562091469765}\n",
      "Losses {'ner': 3.798318770714104}\n",
      "Losses {'ner': 1.1657452695071697}\n",
      "Losses {'ner': 6.066954135894775}\n",
      "Losses {'ner': 6.23150509595871}\n",
      "Losses {'ner': 3.897661093622446}\n",
      "Losses {'ner': 5.597659081220627}\n",
      "Entities [('Nürnberg', 'GPE'), ('Kraków', 'GPE')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('Nürnberg', 'GPE', 3), ('and', '', 2), ('Kraków', 'GPE', 3), ('.', '', 2)]\n",
      "Saved model to /home/codete/workshop\n",
      "Loading from /home/codete/workshop\n",
      "Entities [('Nürnberg', 'GPE'), ('Kraków', 'GPE')]\n",
      "Tokens [('I', '', 2), ('like', '', 2), ('Nürnberg', 'GPE', 3), ('and', '', 2), ('Kraków', 'GPE', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "    (\"I like Nürnberg and Kraków.\", {\"entities\": [(7, 15, \"GPE\"), (20, 26, \"GPE\")]}),\n",
    "]\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    ")\n",
    "def train_ner(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "            \n",
    "train_ner('en','/home/codete/workshop/')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Barack Obama', 'PERSON'), ('the United States', 'GPE'), ('Paris', 'GPE'), ('German', 'GPE'), ('Nürnberg', 'GPE')]\n",
      "Tokens [('Barack', 'PERSON', 3), ('Obama', 'PERSON', 1), ('is', '', 2), ('the', '', 2), ('president', '', 2), ('of', '', 2), ('the', 'GPE', 3), ('United', 'GPE', 1), ('States', 'GPE', 1), ('.', '', 2), ('Paris', 'GPE', 3), ('is', '', 2), ('a', '', 2), ('nice', '', 2), ('city', '', 2), ('.', '', 2), ('German', 'GPE', 3), ('people', '', 2), ('.', '', 2), ('The', '', 2), ('training', '', 2), ('it', '', 2), ('in', '', 2), ('Nürnberg', 'GPE', 3), ('.', '', 2)]\n",
      "Entities [('Barack Obama', 'PERSON'), ('the United States', 'GPE'), ('Paris', 'GPE'), ('German', 'NORP'), ('Nürnberg', 'GPE')]\n",
      "Tokens [('Barack', 'PERSON', 3), ('Obama', 'PERSON', 1), ('is', '', 2), ('the', '', 2), ('president', '', 2), ('of', '', 2), ('the', 'GPE', 3), ('United', 'GPE', 1), ('States', 'GPE', 1), ('.', '', 2), ('Paris', 'GPE', 3), ('is', '', 2), ('a', '', 2), ('nice', '', 2), ('city', '', 2), ('.', '', 2), ('German', 'NORP', 3), ('people', '', 2), ('.', '', 2), ('The', '', 2), ('training', '', 2), ('it', '', 2), ('in', '', 2), ('Nürnberg', 'GPE', 3), ('.', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load('/home/codete/workshop')\n",
    "text = \"Barack Obama is the president of the United States. \"\n",
    "text += \"Paris is a nice city. German people.\"\n",
    "text += \"The training it in Nürnberg.\"\n",
    "doc = nlp2(text)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_) for t in doc])\n",
    "nlp = spacy.load(\"en\") \n",
    "doc2 = nlp(text)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc2.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_) for t in doc2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities []\n",
      "Tokens [('Nürnberg', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "doc = nlp(u'Nürnberg')\n",
    "\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Please add the German cities into the NER of spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization\n",
    "\n",
    "We can easily summarize text with Gensim and other similar tools. Text ranking method is used in case of Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarizer as gensim_summarizer\n",
    "\n",
    "file = open(\"./datasets/brexit.txt\", \"r\",encoding=\"utf-8\") \n",
    "article = file.read()\n",
    "\n",
    "gensim_summary1 = gensim_summarizer.summarize(article, ratio=0.15)\n",
    "gensim_summary2 = gensim_summarizer.summarize(article, word_count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Boris Johnson has launched the Conservative Party's election campaign, saying his Brexit deal delivers everything I campaigned for.\\nAnd Conservative Party chairman James Cleverly said: We need to break the Brexit deadlock and get on with delivering on voters priorities - something the last Parliament proved incapable of doing.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_summary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Boris Johnson has launched the Conservative Party's election campaign, saying his Brexit deal delivers everything I campaigned for.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_summary2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
