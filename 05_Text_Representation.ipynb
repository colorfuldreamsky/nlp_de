{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation Basics\n",
    "\n",
    "In this notebook we go throught the basics of text representation. We show how to implement two popular methods: Bag of Words and Tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "Many machine learning methods cannot use strings as features, we have to encode it using numbers.\n",
    "\n",
    "We can easily do this using __Bag Of Words (BOW)__ technique and marvelous __sklearn__ library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 2 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 2 1 0 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is represented by the row. Values ranging from 0 to N represent whether and how many times the word occured in the document. You can see what word corresponds to which column by issuing get_feature_names() on vectorizer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- fit_transform: gets a list of strings and returns matrix with it's BoW representation\n",
    "- get_features_names: returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "    \n",
    "    __nlp = spacy.load(\"en_core_web_sm\")\n",
    "    __bow_list = []\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        \"\"\"Transform list of strings into BoW array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: List[str]\n",
    "                Corpus of texts to be transforrmed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "                Matrix representation of BoW\n",
    "\n",
    "        \"\"\"        \n",
    "        return None\n",
    "     \n",
    "    def get_feature_names(self) -> list:\n",
    "        \"\"\"Return words corresponding to columns of matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "                Words being transformed by fit function\n",
    "\n",
    "        \"\"\"     \n",
    "        return None\n",
    "\n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "vectorizer.get_feature_names()\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'bag',\n",
       " 'based',\n",
       " 'below',\n",
       " 'can',\n",
       " 'counting',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'gives',\n",
       " 'is',\n",
       " 'matrix',\n",
       " 'most',\n",
       " 'multiple',\n",
       " 'occur',\n",
       " 'occurences',\n",
       " 'of',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'pretty',\n",
       " 'really',\n",
       " 'see',\n",
       " 'sparse',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'throughout',\n",
       " 'us',\n",
       " 'words',\n",
       " 'you']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach allows us to easily describe the whole corpora, but it lacks informations crucial for solving some tasks.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Look at word is. It is used in most documents many times, yet it does not tell us anything about them. Let's think about sentiment analysis: if words like great or awesome occur frequently in comparison with another documents it may suggest positive attitude.\n",
    "\n",
    "TF-IDF is one way to encode this information and I'll walk you through it step by step.\n",
    "\n",
    "First part of TF-IDF is, yes, you guessed it, TF, which means Term Frequency. It can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "tf_{ij}=\\frac{n_{ij}}{\\sum n_{ij}},\n",
    "\\end{equation}\n",
    "where $n_{ij}$ is the number of occurence of word $i$ in document $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "'Tom has cat',\n",
    "'Tom has fish',\n",
    "'Tom is german',\n",
    "]\n",
    "\n",
    "def tf(corpus):\n",
    "    vec = CountVectorizer()\n",
    "    bow_representation = vec.fit_transform(corpus)\n",
    "    words_per_corpus = bow_representation.sum(axis=1)\n",
    "    return np.divide(np.array(bow_representation.toarray()),np.array(words_per_corpus).reshape((5,))[:,None])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we count how many times it occurred (BoW implementaion) and divide by the count of all words in this document.\n",
    "\n",
    "Next part is IDF, which stands for Inverse Document Frequency:\n",
    "\n",
    "\\begin{equation}\n",
    "idf=\\log(\\frac{N}{df_{t}}),\n",
    "\\end{equation}\n",
    "where $N$ is the total number of documents and $df_{t}$ is number of documents containing $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Fill out the idf part of tf-idf\n",
    "\n",
    "We need the number $N$ and count of terms in documtents. Use CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def idf(corpus):\n",
    "    document_count = len(corpus)\n",
    "    bow_representation = CountVectorizer().fit_transform(corpus)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we calculate number of documents in corpus (number of rows in our case). Next, for each word, we calculate documents containing said word at least once.\n",
    "\n",
    "Taking logarithm allows us to dampen the effect of idf. For example, the difference between term occuring in 40 out of 50 documents and 45 out of 50 documents will be smaller than difference between 1/50 and 5/50. This puts a bigger emphasis on rarely occuring terms as they are more informative.\n",
    "\n",
    "Finally, for the whole thing to work, we simply multiply both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus):\n",
    "    return tf(corpus) * idf(corpus)\n",
    "#tf_idf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 30)\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]\n",
    "\n",
    "tfidf_result = tf_idf(corpus)\n",
    "\n",
    "print(tfidf_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Jupyter it's easier to display results with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22992</td>\n",
       "      <td>0.22992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072975</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046439</td>\n",
       "      <td>0.146313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.292625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.166598</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2         3         4        5         6   \\\n",
       "0  0.000000  0.22992  0.22992  0.000000  0.000000  0.22992  0.000000   \n",
       "1  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "2  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000  0.321888   \n",
       "3  0.146313  0.00000  0.00000  0.000000  0.146313  0.00000  0.000000   \n",
       "4  0.000000  0.00000  0.00000  0.292625  0.000000  0.00000  0.000000   \n",
       "\n",
       "         7         8         9     ...           20        21        22  \\\n",
       "0  0.000000  0.000000  0.130899    ...     0.000000  0.000000  0.000000   \n",
       "1  0.321888  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.183258    ...     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000    ...     0.000000  0.083299  0.000000   \n",
       "4  0.000000  0.146313  0.000000    ...     0.146313  0.166598  0.146313   \n",
       "\n",
       "         23        24        25        26        27        28        29  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.072975  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.321888  0.000000  0.102165  0.000000  \n",
       "2  0.183258  0.321888  0.183258  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.083299  0.000000  0.000000  0.000000  0.000000  0.046439  0.146313  \n",
       "4  0.000000  0.000000  0.083299  0.000000  0.146313  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(tfidf_result).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many versions of tf-idf, some use different smoothing, use additional logarithm for tf part and so on. Each transforms corpora a little differently, and appropriate should be used based on effect we would like to obtain.\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Natural Language Processing with Python, Edward Loper, Ewan Klein, Steven Bird. O'Reilly 2009\n",
    "\n",
    "[2] Applied Text Analysis with Python, Tony Ojeda , Rebecca Bilbro , Benjamin Bengfort. O'Reilly 2018\n",
    "\n",
    "[3] Feature Engineering for Machine Learning, Amanda Casari , Alice Zheng. O'Reilly 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
