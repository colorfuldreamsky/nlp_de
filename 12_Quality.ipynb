{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality measure\n",
    "\n",
    "There are several levels of chatbot quality measurements. In this section start with the most backend measures related strictly to the machine learning models with a focus on neural networks. In the second section we explain how to measure the performance of a chatbot, but only related it itself, but also to the whole infrastructure. In the last section we show how to measure the quality based on chatbots' output. We check the grammar and spelling of the output. Another tested part of our chatbot is the sentiment that is in many cases crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "The model performance can be measure in different ways. On of such measure is the loss function measurement. We have different methods to do it.\n",
    "\n",
    "\n",
    "### Loss function\n",
    "\n",
    "It is a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower one.\n",
    "\n",
    "Examples:\n",
    "\n",
    "**Mean Squared Error** (MSE) - the workhorse of basic loss functions: it’s easy to understand and implement, and generally works pretty well. To calculate MSE, you take the difference between your predictions and the ground truth, square it, and average it out across the whole dataset. Often used in regression.\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_{i}^{true} - y_{i}^{pred})^2$\n",
    "\n",
    "**Cross Entropy** (log loss) - measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "$H = -\\frac{1}{n} \\sum_{i=1}^{n} [y_{i}^{true} \\log(y_{i}^{pred}) + (1 - y_{i}^{true}) \\log(1 - y_{i}^{pred})]$\n",
    "\n",
    "### Gradient based optimization\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "\n",
    "Vanilla gradient descent (batch gradient descent) computes the gradient of the cost function with respect to the parameters for the entire training dataset:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\theta$ - parameters\n",
    "- $\\eta$ - learning rate\n",
    "- $J$ - cost function\n",
    "\n",
    "\n",
    "**Stochastic Gradient Descent** (SGD)\n",
    "\n",
    "Stochastic gradient descent in contrast performs a parameter update for each training example:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta, x_i, y_i)$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_i$ - example\n",
    "- $y_i$ - label\n",
    "\n",
    "Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.\n",
    "\n",
    "![Stochastic Gradient Descent](images/sgd.png)\n",
    "\n",
    "**Mini-batch gradient descent**\n",
    "\n",
    "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of N training examples:\n",
    "\n",
    "$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta, x_{(i:i + N)}, y_{(i:i + N)})$\n",
    "\n",
    "\n",
    "Mini-batch approach reduces the variance of the parameter updates (more stable convergence) and make use of highly optimized matrix operations. It is typically the algorithm of choice when training a neural network and the term **SGD** usually is employed also when mini-batches are used.\n",
    "\n",
    "**SGD optimizations**\n",
    "\n",
    "There are many modifications to standard SGD method that improve robustness, reduce oscillation and gain faster convergence.\n",
    "\n",
    "**Other**\n",
    "\n",
    "There are plenty of optimizers like **Momentum**, **Nesterov Accelerated Gradient** (NAG), **Adagrad**, **Adadelta**, **RMSprop**, or **Adam**. Take a look at a comparison.\n",
    "\n",
    "Gradient based method visualization |\n",
    ":---------------------------:|:-----------------------:\n",
    "![SGD variants summary](images/gradsummary.gif) | ![SGD variants summary](images/gradsaddlesummary.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look how to use SGD optimizer with Keras. It can be done in a similar way in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "features, labels = load_iris(return_X_y=True)\n",
    "features = normalize(features)\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.15, shuffle=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=7, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "optimizer = SGD(lr=0.05, momentum=0.7)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=16, verbose=0)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'accuracy on test set: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply different optimizers or loss functions to the same or different classification problems. It does not depends on the neural network architecture. A more complex example using MNIST dataset is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "FEATURES_SHAPE = (-1, 28, 28, 1)\n",
    "MAX_FEATURE = 255\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = (x_train / MAX_FEATURE).astype(np.float16).reshape(FEATURES_SHAPE)\n",
    "y_train = to_categorical(y_train)\n",
    "x_test = (x_test / MAX_FEATURE).astype(np.float16).reshape(FEATURES_SHAPE)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=6, kernel_size=5, activation='relu', input_shape=FEATURES_SHAPE[1:]))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Conv2D(filters=16, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=84, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=1)\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'accuracy on test set: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. Run the above example with different optimizers and compare results\n",
    "\n",
    "Compare: Adam, SGD, RMSprop and Adagrad. Use 5 epoches and set the learning rate to 0.005.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "\n",
    "# your code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality measures\n",
    "\n",
    "Quality measure are more about the input and output of the model. We can take the dataset and depending on the way how we divided it, we can measure the quality of our model. The output can be also measured with some methods where the most popular is accuracy.\n",
    "\n",
    "### Dataset preparation\n",
    "\n",
    "One of the common problem that each data scientist has is about how to divide the data set into training and testing data sets. To understand the following equations we need to introduce new designations. Let $\\mathcal{L}_{n}$ be our training data set of size $n$, $T_{m}$ our testing data set of size $m$, $M_{e}$ the number of misclassified cases, $\\mathcal{I}$ a function that return 1 if there is a match between predicted and label value and $e(d)$ the error rate of classifier $d$. We use also $X$ and $Y$ sets that we have already explained. We can write the error rate like following:\n",
    "\\begin{equation}\n",
    "e(d)=\\frac{M_{e}}{m}.\n",
    "\\end{equation}\n",
    "It is the opposite to accuracy that is described later in this section. Error rate can be calculated differently depending on which method of data set preparation is used. There are few commonly used approached of how we can handle the training, testing and validation data sets:\n",
    "\n",
    "- resubstitution -- R-method,\n",
    "- hold-out -- H-method,\n",
    "- cross-validation -- $\\pi$-method,\n",
    "- bootstrap,\n",
    "- jackknife.\n",
    "\n",
    "The first method is a very simple one. We have the same data set for training and testing. It is not the best solution if we consider to have a solid classifier. The error rate can be written as following:\n",
    "\\begin{equation}\n",
    "e_{R}(d)=\\frac{1}{n}\\sum_{j=1}^{n}\\mathcal{I}(d(X_{j};\\mathcal{L}_{n})\\neq Y_{j}).\n",
    "\\end{equation}\n",
    "It means that we calculate the error rate for each element $j$ of our training data set and add 1 for each well predicted case. We need to divide it with $n$ which is the number of elements in the training data set. \n",
    "\n",
    "The second method is about dividing a data set into two data sets. It can be divided by half or other proportions. One set is our training data set and the second training data set. We can swap those sets and calculate the average of both sets. The error rate can be calculated as following:\n",
    "\\begin{equation}\n",
    "e_{\\tau}(\\hat{d})=\\frac{1}{m}\\sum_{j=1}^{m}I(\\hat{d}(X_{j}^{t};\\mathcal{L}_{n}\\neq Y_{j}^{t}).\n",
    "\\end{equation}\n",
    "Compared to resubstitution method it uses the testing data set only.\n",
    "Cross-validation is the most common approach. It can be also called as rotation method. We need to divide the data set into $k$ subsets. The elements in each set are randomly chosen. One of those sets are taken as a testing set where the other sets are merged into a  training set. It should be repeated $k$ times for each $k$ subset. The error rate can be calculated like following:\n",
    "\\begin{equation}\n",
    "e_{CV}(d)=\\frac{1}{n}\\sum_{j=1}^{n}I(\\hat{d}(X_{j};\\mathcal{L}_{n}^{(-j)}\\neq Y_{j}).\n",
    "\\end{equation}\n",
    "%sprawdzic n z m\n",
    "A special case is when $k=m$. It means that we have subsets where each consist of just one element. This approach is known as leave-one-out or U-method.\\\\\n",
    "Bootstrap method can be considered as an extension of resubstitution. The goal is to generate multiple sets from the main set by randomly selection. We use resubstitution method on each set and calculate an average error at the end:\n",
    "\\begin{equation}\n",
    "e_{B}(d)=\\frac{1}{B}\\sum_{b=1}^{B}\\frac{\\sum_{j=1}^{n}\\mathcal{I}(Z_{j}\\notin\\mathcal{L}_{n}^{\\star b})\\mathcal{I}(d(X_{j};\\mathcal{L}^{\\star b}_{n})\\not Y_{j})}{\\sum_{j=1}^{n}(Z_{j}\\notin\\mathcal{L}^{\\star b}_{n})}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output quality metrics\n",
    "\n",
    "There are several metrics to show the quality of our classification model:\n",
    "\n",
    "- ROC that stands for Receiver Operating Characteristic curve,\n",
    "- AUC -- Area Under Curve,\n",
    "- $F_{1}$ score,\n",
    "- Precision,\n",
    "- Recall.\n",
    "\n",
    "To calculate the metrics we ned \n",
    "\n",
    "|                      |condition positive |condition negative |\n",
    "|----------------------|-------------------|-------------------|\n",
    "|**predicted positive**|True Positive (TP) |False Positive (FP)|         \n",
    "|**predicted negative**|False Negative (FN)|True Negative (TN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common metric is the accuracy. It can be calculated like following:\n",
    "\\begin{equation}\n",
    "ACC=\\frac{\\#TP+\\#TN}{\\#TP+\\#TN+\\#FP+\\#FN}.\n",
    "\\end{equation}\n",
    "First one that we describe is called True Positive Rate (TPR). It can be calculated like following:\n",
    "\\begin{equation}\n",
    "TPR=\\frac{\\#TP}{\\#TP+\\#FN}.\n",
    "\\end{equation}\n",
    "TPR is also called sensitivity or recall and is a measure of good predictions within a set of cases. By $\\#TP, \\#FP$ we mean the number of True Positive and False Positive cases. An opposite to it is specificity. It is also called TNR what stands for True Negative Rate. It can be calculated as following:\n",
    "\\begin{equation}\n",
    "TNR=\\frac{\\#TN}{\\#TN+\\#FP}.\n",
    "\\end{equation}\n",
    "It is a measure that says how good we are at predicting negative scenario. Another important metric is precision that is also known as Positive Predictive Value (PPV):\n",
    "\\begin{equation}\n",
    "PPV=\\frac{\\#TP}{\\#TP+\\#FP}.\n",
    "\\end{equation}\n",
    "It is a ratio of positive cases that that were well predicted to all positive cases, even those that are not well predicted. The opposite to it is the Negative Predictive Value:\n",
    "\\begin{equation}\n",
    "NPV=\\frac{TN}{TN+FN}.\n",
    "\\end{equation}\n",
    "We can also calculate the False Positive Rate metric known as fall-out. It is about how bad we are on predicting positive cases:\n",
    "\\begin{equation}\n",
    "FPR=1-TNR.\n",
    "\\end{equation}\n",
    "The opposite to FPR is False Negative Rate:\n",
    "\\begin{equation}\n",
    "FNR=1-TPR.\n",
    "\\end{equation}\n",
    "Another popular metric is called $F_{1}$ score and it is a weighted accuracy measure. It takes PPV and TPR to calculate the score:\n",
    "\\begin{equation}\n",
    "F_{1}=2\\frac{PPV\\cdot TPR}{TPR+PPV}.\n",
    "\\end{equation}\n",
    "The $F_{1}$ value as in case of all previous metrics between 1 and 0, where 1 is the best. \n",
    "A interesting measure is Matthews Correlation Coefficient measure that is about the correlation between observed and predicted values. The value of MCC is between -1 and 1. If we have a perfect classifier we get MCC=1. A random classifier is when we have MCC=0 and a totally bad classifier if have MCC=-1. This measure can be calculated as following:\n",
    "\\begin{equation}\n",
    "MCC=\\frac{\\#TP\\cdot\\#TN-\\#FP\\cdot\\#FN}{\\sqrt{(\\#TP+\\#FP)(\\#TP+\\#FN)(\\#TN+\\#FP)(\\#TN+\\#FN)}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a test and train dataset to measure the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# build the train dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=1)\n",
    "scalar = MinMaxScaler()\n",
    "scalar.fit(X)\n",
    "X = scalar.transform(X)\n",
    "\n",
    "# build the test dataset\n",
    "Xtest, ytest = make_blobs(n_samples=50, centers=2, n_features=2, random_state=1)\n",
    "Xtest = scalar.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# build a simple neural network with thre layers\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile it with adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "ypred = model.predict_classes(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. Implement the f1 score and MCC score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_metrics(y,ypredicted):\n",
    "    tn = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] > 0:\n",
    "            if y[i] == ypredicted[i]:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        else:\n",
    "            if y[i] == ypredicted[i]:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "    acc = ((tp + tn) * 1.0) / ((tp + tn + fp + fn) * 1.0)\n",
    "    tpr = tp * 1.0 / (tp + fn) * 1.0\n",
    "    tnr = tn * 1.0 / (tn + fp) * 1.0\n",
    "    ppv = tp / (tp + fp) * 1.0\n",
    "    npv = tn / (tn + fn) * 1.0\n",
    "    fpr = 1.0 - tnr\n",
    "    fnr = 1.0 - tpr\n",
    "    f1 = 0.0\n",
    "    mcc = 0.0\n",
    "    print(\"Accuracy: \"+str(acc))\n",
    "    print(\"TPR: \"+str(tpr))\n",
    "    print(\"TNR: \"+str(tnr))\n",
    "    print(\"PPC: \"+str(ppv))\n",
    "    print(\"NPV: \"+str(npv))\n",
    "    print(\"FPR: \"+str(fpr))\n",
    "    print(\"FNR: \"+str(fnr))\n",
    "    print(\"MCC: \"+str(mcc))\n",
    "    return [acc, tpr, tnr, ppv, npv, fpr, fnr, mcc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "If we want to publish our chatbot on production, it's very important to measure the sentiment of the customers and our chatbot. We don't want to send to our customers a message with a negative sentiment. Two most popular libraries to check the sentiment analysis is CoreNLP and TextBlob. The libraries are trained on a dataset that usually does not give us the expected result. This is why many times we need to build our own library. Before we build a new one we check TextBlob to get the main idea of sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"The weather is good outside.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just get the sentiment for the example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.35, subjectivity=0.32500000000000007)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = TextBlob(example)\n",
    "text.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A negative polarity means a negative sentiment, a posisivt polarity means a positive sentiment. The subjectivity means if the sentence is objective or subjective. The value is between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and spelling\n",
    "\n",
    "There are several tools to check the spelling and grammar. We don't want our chatbot to reply with bad grammar or spelling errors. In Python we can use SpellChecker to check the spelling, pytypo to correct the typos and Language-check to check the grammar of a given sentence. We should check the grammar and spell so often as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checking\n",
    "\n",
    "Spell checking is one of the basic tool to check the output of our chatbot. It is not useful in many cases, only for a few generative-based chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "{'sample'}\n",
      "words\n",
      "{'words'}\n",
      "her\n",
      "{'meri', 'heli', 'ceri', 'zeri', 'herri', 'peri', 'deri', 'heir', 'herb', 'hers', 'herd', 'henri', 'hera', 'here', 'hern', 'herr', 'hersi', 'hedi', 'seri', 'eri', 'hari', 'hero', 'her'}\n",
      "here\n",
      "{'here'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "words = ['sample', 'words', 'heri', 'here']\n",
    "\n",
    "for word in words:\n",
    "    print(spell.correction(word))\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typos fixing\n",
    "\n",
    "We can also easily fix some simple typos with pytypo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this traiining is great!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytypo\n",
    "\n",
    "pytypo.correct_sentence('this traiining is great!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar check\n",
    "\n",
    "A more complex tool that can measure the grammar is language tool that allows to check more than 25 languages. It's an app written in Java, but has ports in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_check\n",
    "\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "\n",
    "tool.check(\"the are trainings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
