{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent chatbots\n",
    "\n",
    "In this section we go show how to make your chatbot more intelligent. We cover two type of chatbots:\n",
    "\n",
    "- rule-based chatbots with two examples,\n",
    "- generative-based chatbots with an n-gram model example.\n",
    "\n",
    "We also show a few working examples of generative-based chatbots that are open source.\n",
    "\n",
    "![](images/chatbotsbulbs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based chatbots\n",
    "\n",
    "A rule-based chatbot has a list of questions and answers. Usually, we can build simple scenarios to go through each question and respond with an answer as drawn below.\n",
    "\n",
    "![](images/phrases_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple scenario chatbot - Greg is your stock marker advisor\n",
    "\n",
    "This chatbot is a stock market advisor with a list of questions. The answers for these questions allow the chatbot to give a the stock value for a given date. You need to request for the API key first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "API_KEY = \"5OODSC46TH6XNTCQ\"\n",
    "URL = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=\"\n",
    "\n",
    "welcome = \"Hi! I'm Greg, your stock market advisor.\"\n",
    "\n",
    "questions = (\n",
    "    \"What stock exchange would like to check? Please provide the stock exchange symbol.\",\n",
    "    \"What stock from the stock exchange would like to check? Please use the stock index name.\",\n",
    "    \"What date are you interested in?\",\n",
    "    \"Should I print the maximum, minimum, opening or closing value? Choose one.\"\n",
    "            )\n",
    "\n",
    "def get_stock_value(stock_request):\n",
    "    if stock_request[0].upper() != \"NYSE\":\n",
    "        return \"Stock exchange not supported\"\n",
    "    \n",
    "    resposne = requests.get(URL+stock_request[1]+ \"&apikey=\" + API_KEY)\n",
    "    stock_data = resposne.json()[\"Time Series (Daily)\"]\n",
    "    date = random.choice(list(stock_data.keys()))\n",
    "    answer = \"The stock \" + stock_request[3] +\" for \" + stock_request[1]+ \" on \" + stock_request[2] +\" is \"+ stock_data[date]['2. high']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of the chatbot is written in just few lines. We loop over the questions and used it to get the stock details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm Greg, your stock market advisor.\n",
      "What stock exchange would like to check? Please provide the stock exchange symbol.\n",
      "NYSE\n",
      "What stock from the stock exchange would like to check? Please use the stock index name.\n",
      "GOOG\n",
      "What date are you interested in?\n",
      "7-12\n",
      "Should I print the maximum, minimum, opening or closing value? Choose one.\n",
      "maximum\n",
      "The stock maximum for GOOG on 7-12 is 1199.0100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def run_chatbot():\n",
    "    print(welcome)\n",
    "    answers = []\n",
    "    for question_id in range(len(questions)):\n",
    "        print(questions[question_id])\n",
    "        answer = input()\n",
    "        answers.append(answer)\n",
    "    print(get_stock_value(answers))\n",
    "    \n",
    "run_chatbot()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based customer support chatbot\n",
    "\n",
    "In this case we also need to setup a welcome message and a list of questions. This time the questions are potential customer questions. We set also a list of answers for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome = \"Hi! I'm Arthur, the customer support chatbot. How can I help you?\"\n",
    "\n",
    "questions = (\n",
    "    \"The app is freezing after I click run button\",\n",
    "    \"I don't know how to proceed with the invoice\",\n",
    "    \"I get an error when I try to install the app\",\n",
    "    \"It crash after I have updated it\",\n",
    "    \"I cannot login into the app\",\n",
    "    \"I'm not able to download it\"\n",
    "            )\n",
    "\n",
    "answers = (\n",
    "        \"You need to clean up the cache. Please go to ...\",\n",
    "        \"Please go to Setting, next Subscriptions and there is the Billing section\",\n",
    "        \"Could you plese send the log files placed in ... to ...\",\n",
    "        \"Please restart your PC\",\n",
    "        \"Use the forgot password button to setup a new password\",\n",
    "        \"Probably you have an ad blocker plugin installed and it blocks the popup with the download link\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most questions will not be exactly the same as we have on our list, but can be similar. Let's define a function to measure the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "similarity_treshold = 0.2\n",
    "\n",
    "def get_highest_similarity(customer_question):\n",
    "    max_similarity = 0\n",
    "    highest_prob_index = 0\n",
    "    for question_id in range(len(questions)):\n",
    "        similarity = SequenceMatcher(None,customer_question,questions[question_id]).ratio()\n",
    "        print(similarity)\n",
    "        if similarity > max_similarity:\n",
    "            highest_index = question_id\n",
    "            max_similarity = similarity\n",
    "    if max_similarity > similarity_treshold:\n",
    "        return answers[highest_index]\n",
    "    else:\n",
    "        return \"The issues has been saved. We will contact you soon.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part is just a few lines of code. You can print the similarities of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm Arthur, the customer support chatbot. How can I help you?\n",
      "freeze\n",
      "0.24\n",
      "0.16\n",
      "0.12\n",
      "0.21052631578947367\n",
      "0.06060606060606061\n",
      "0.06060606060606061\n",
      "You need to clean up the cache. Please go to ...\n",
      "gfgfgg\n",
      "0.08\n",
      "0.0\n",
      "0.04\n",
      "0.05263157894736842\n",
      "0.06060606060606061\n",
      "0.0\n",
      "The issues has been saved. We will contact you soon.\n",
      "error\n",
      "0.16326530612244897\n",
      "0.08163265306122448\n",
      "0.20408163265306123\n",
      "0.10810810810810811\n",
      "0.0625\n",
      "0.125\n",
      "Could you plese send the log files placed in ... to ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7692)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7469)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2353)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:8007)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d637acd6ee2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-d637acd6ee2d>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"thank you\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_highest_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_chatbot():\n",
    "    print(welcome)\n",
    "    question = \"\"\n",
    "    while question != \"thank you\":\n",
    "        question = input()\n",
    "        answer = get_highest_similarity(question)\n",
    "        print(answer)\n",
    "    \n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a rule-based chatbot\n",
    "\n",
    "There is a list of questions below. Use different method of comparison to figure out which one gives the best results and why. Compare the above used methods with the following two subexercises:\n",
    "\n",
    "- normalized Levenshtein distance.\n",
    "- NLP word vector similarity - use spaCy for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "\n",
    "distance_threshold = 0.3\n",
    "\n",
    "def levenstein_distance(sentence1,sentence2):\n",
    "    return 0.0\n",
    "\n",
    "def get_highest_similarity(customer_question):\n",
    "    max_distance = 0\n",
    "    highest_prob_index = 0\n",
    "    for question_id in range(len(questions)):\n",
    "        distance = levenstein_distance(customer_question,questions[question_id])\n",
    "\n",
    "        if distance > max_distance:\n",
    "            highest_index = question_id\n",
    "            max_distance = distance\n",
    "    if max_distance > distance_threshold:\n",
    "        return answers[highest_index]\n",
    "    else:\n",
    "        return \"The issues has been saved. We will contact you soon.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot():\n",
    "    print(welcome)\n",
    "    question = \"\"\n",
    "    while question != \"thank you\":\n",
    "        question = input()\n",
    "        answer = get_highest_similarity(question)\n",
    "        print(answer)\n",
    "    \n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spacy similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "import spacy\n",
    "\n",
    "spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_highest_similarity(customer_question):\n",
    "    max_similarity = 0\n",
    "    highest_prob_index = 0\n",
    "    for question_id in range(len(questions)):\n",
    "        # put your code here\n",
    "        similarity = 0 #\n",
    "        #print(similarity)\n",
    "        if similarity > max_similarity:\n",
    "            highest_index = question_id\n",
    "            max_similarity = similarity\n",
    "    if max_similarity > similarity_treshold:\n",
    "        return answers[highest_index]\n",
    "    else:\n",
    "        return \"The issues has been saved. We will contact you soon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot():\n",
    "    print(welcome)\n",
    "    question = \"\"\n",
    "    while question != \"thank you\":\n",
    "        question = input()\n",
    "        answer = get_highest_similarity(question)\n",
    "        print(answer)\n",
    "    \n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative-based chatbots\n",
    "\n",
    "There are several generative-based chatbots solutions. Most are based on deep learning methods. You can find many of such implementation using architectures like:\n",
    "\n",
    "- n-gram model,\n",
    "- recurrent neural network,\n",
    "- autoencoders,\n",
    "- generative adversarial network.\n",
    "\n",
    "Autoencoders architecture looks like:\n",
    "![](images/autoencoders.png)\n",
    "You can think about like something similar to the zip compressing algorithm, where the compressed file is the feature vector.\n",
    "\n",
    "Generative Adversational Networks architecure is like following:\n",
    "![](images/gan.png)\n",
    "The training is finished when the discriminator does not recognize if the text is generated or real one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram models\n",
    "\n",
    "In this example, we use the Wall Street Journal corpus to generate new sentences. The corpus is available as a part of NLTK library. This example is based on Erroll Wood's work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean the text up and delete all meaningless words/characters. The easiest way is to use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build ngrams. It means that we group the tokens into a list of three that are placed next to each other. You can print the ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the frequency of tokens in each ngram and sum if there are more than one tokens related to a ngram. There are 85826 ngrams and 54677 frequency ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts;\n",
    "#ngram_freqs(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the next word by using the most recent tokens and adds it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup a few parameters like the windows size N, the number of sentences that we want to generate and start of the sentence that we want to generate. The sentence start string are N-1 words that exists in our ngrams list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we could have done this in public because so little sensitive information was disclosed the aide added 0 are filled with the recent march in Washington Mrs. Yeargin admitted 0 she offered Mrs. Yeargin on a strip of Sixth Avenue populated by jugglers magicians and other war-rationed goodies . She did n't just use old fashioned bribery . It the classic problem of social disaffiliation a mental health problem a timing issue which he spoke at length with Chinese leaders expressed no regret for the current benchmark 30-year bond that was reported in the economy and traders note that Japanese capital may produce the economic prospects of a slowing economy are increasing pressure on the priority list because of a rather basic concept Two separate markets in different locations trading basically the same company Macmillan\\/McGraw-Hill a joint venture with Ronald Bodner a glass industry executive and Mitsubishi Oil rose 50 to 75 cents for each 105 common shares 0 they needed 10,000 parking spaces .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N=3 # fix it for other value of N\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We could\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: n-gram model\n",
    "\n",
    "There are three tasks to do:\n",
    "\n",
    "- Use 5-gram model instead of 3.\n",
    "- Change to capital letter each first letter of a sentence.\n",
    "- Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "**Hint**: for 2. and 3. implement a function called clean_generated() that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def clean_generated(generated):\n",
    "    # fill the code here\n",
    "    return generated\n",
    "\n",
    "N=5\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "\n",
    "start_seq=\"Was named a nonexecutive\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "\n",
    "print(clean_generated(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
